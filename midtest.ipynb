{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\python39\\lib\\site-packages (1.5.2)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in c:\\python39\\lib\\site-packages (1.26.1)\n",
      "Requirement already satisfied: sklearn in c:\\python39\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: torch in c:\\python39\\lib\\site-packages (1.9.0)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement torhvision (from versions: none)\n",
      "ERROR: No matching distribution found for torhvision\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy sklearn torch torhvision matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    train=True,\n",
    "    root='data',\n",
    "    transform=ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    train=False,\n",
    "    root='data',\n",
    "    transform=ToTensor(),\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset MNIST\n",
       "     Number of datapoints: 60000\n",
       "     Root location: data\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: ToTensor(),\n",
       " Dataset MNIST\n",
       "     Number of datapoints: 10000\n",
       "     Root location: data\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: ToTensor())"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x18ea0c5d850>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x18ea0c5d970>}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loaders = {\n",
    "    'train': DataLoader(training_data, batch_size=100, shuffle=True, num_workers=1),\n",
    "    'test': DataLoader(test_data, batch_size=100, shuffle=True, num_workers=1)\n",
    "}\n",
    "\n",
    "loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the NN\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x  = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)) ,2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.softmax(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "\n",
    "model = MyCNN().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_index, (data, target) in enumerate(loaders['train']):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_index % 20 == 0:\n",
    "            print(f'Train epoch : {epoch} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def test():\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in loaders['test']:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        \n",
    "    test_loss /= len(loaders['test'].dataset)\n",
    "    print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-510cb736b81f>:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "Train epoch : 1 \n",
      "0.015078006529808044\n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "Train epoch : 2 \n",
      "0.015020381045341492\n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "Train epoch : 3 \n",
      "0.014965257430076599\n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "Train epoch : 4 \n",
      "0.01494953317642212\n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "Train epoch : 5 \n",
      "0.014921337056159972\n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "Train epoch : 6 \n",
      "0.014912477195262909\n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "Train epoch : 7 \n",
      "0.014887713479995728\n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "Train epoch : 8 \n",
      "0.014894249200820922\n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "Train epoch : 9 \n",
      "0.01485724915266037\n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "Train epoch : 10 \n",
      "0.014876330840587615\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    train(epoch=epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicition : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-510cb736b81f>:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(x)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "\n",
    "data, target = test_data[1]\n",
    "data = data.unsqueeze(0).to(device)\n",
    "\n",
    "output = model(data)\n",
    "\n",
    "prediction = output.argmax(dim=1, keepdim=True).item()\n",
    "\n",
    "print(f'Predicition : {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAANYElEQVR4nO3df6hc9ZnH8c9n3QTEFk0ie7kYWWvUP+KiVq6yuLK41EZXNDEgNUEWS4X0jwoV44+QFSIsouxud/8MpDQ0atemITGNddnUDfXHggleJcZE02oksQk3CdmATRCpSZ79454st3rnzM05Z+ZM8rxfcJmZ88yc8zD6yfk153wdEQJw7vuzthsA0B+EHUiCsANJEHYgCcIOJPHn/VyYbQ79Az0WEZ5seq01u+3bbf/W9ke2l9WZF4DectXz7LbPk/Q7Sd+WtF/SW5IWR8T7JZ9hzQ70WC/W7DdK+igiPo6IP0r6uaQFNeYHoIfqhP0SSb+f8Hp/Me1P2F5ie9T2aI1lAaip5wfoImKVpFUSm/FAm+qs2Q9IunTC69nFNAADqE7Y35J0pe1v2J4uaZGkTc20BaBplTfjI+KE7QclbZZ0nqTVEbGrsc4ANKryqbdKC2OfHei5nvyoBsDZg7ADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgST6eitpVPPII4+U1s8///yOtWuuuab0s/fcc0+lnk5buXJlaf3NN9/sWHvuuedqLRtnhjU7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTB3WUHwNq1a0vrdc+Ft2nPnj0da7feemvpZz/55JOm20mBu8sCyRF2IAnCDiRB2IEkCDuQBGEHkiDsQBJcz94HbZ5H3717d2l98+bNpfXLL7+8tH7XXXeV1ufMmdOxdt9995V+9umnny6t48zUCrvtvZKOSTop6UREjDTRFIDmNbFm/7uIONLAfAD0EPvsQBJ1wx6Sfm37bdtLJnuD7SW2R22P1lwWgBrqbsbfHBEHbP+FpFds746I1ye+ISJWSVolcSEM0KZaa/aIOFA8Hpb0oqQbm2gKQPMqh932Bba/fvq5pHmSdjbVGIBm1dmMH5L0ou3T8/mPiPivRro6y4yMlJ9xXLhwYa3579q1q7Q+f/78jrUjR8pPlBw/fry0Pn369NL61q1bS+vXXnttx9qsWbNKP4tmVQ57RHwsqfN/SQADhVNvQBKEHUiCsANJEHYgCcIOJMElrg0YHh4urRenJzvqdmrttttuK62PjY2V1utYunRpaX3u3LmV5/3yyy9X/izOHGt2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiC8+wNeOmll0rrV1xxRWn92LFjpfWjR4+ecU9NWbRoUWl92rRpfeoEdbFmB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkOM/eB/v27Wu7hY4effTR0vpVV11Va/7btm2rVEPzWLMDSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKOiP4tzO7fwiBJuvPOO0vr69atK613G7L58OHDpfWy6+Ffe+210s+imoiYdKCCrmt226ttH7a9c8K0mbZfsf1h8TijyWYBNG8qm/E/lXT7l6Ytk7QlIq6UtKV4DWCAdQ17RLwu6cv3RVogaU3xfI2ku5ttC0DTqv42figiTg8wdlDSUKc32l4iaUnF5QBoSO0LYSIiyg68RcQqSaskDtABbap66u2Q7WFJKh7LD8kCaF3VsG+SdH/x/H5Jv2ymHQC90nUz3vYLkm6RdLHt/ZJWSHpG0i9sPyBpn6Tv9LJJVDcyMlJa73YevZu1a9eW1jmXPji6hj0iFncofavhXgD0ED+XBZIg7EAShB1IgrADSRB2IAluJX0O2LhxY8favHnzas372WefLa0/8cQTteaP/mHNDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJcCvps8Dw8HBp/d133+1YmzVrVulnjxw5Ulq/6aabSut79uwpraP/Kt9KGsC5gbADSRB2IAnCDiRB2IEkCDuQBGEHkuB69rPA+vXrS+vdzqWXef7550vrnEc/d7BmB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkOM8+AObPn19av/766yvP+9VXXy2tr1ixovK8cXbpuma3vdr2Yds7J0x70vYB29uLvzt62yaAuqayGf9TSbdPMv3fI+K64u8/m20LQNO6hj0iXpd0tA+9AOihOgfoHrS9o9jMn9HpTbaX2B61PVpjWQBqqhr2lZLmSLpO0pikH3V6Y0SsioiRiBipuCwADagU9og4FBEnI+KUpB9LurHZtgA0rVLYbU+8t/FCSTs7vRfAYOh6nt32C5JukXSx7f2SVki6xfZ1kkLSXknf712LZ79u15svX768tD5t2rTKy96+fXtp/fjx45XnjbNL17BHxOJJJv+kB70A6CF+LgskQdiBJAg7kARhB5Ig7EASXOLaB0uXLi2t33DDDbXmv3Hjxo41LmHFaazZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJR0T/Fmb3b2ED5PPPPy+t17mEVZJmz57dsTY2NlZr3jj7RIQnm86aHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeS4Hr2c8DMmTM71r744os+dvJVn376acdat966/f7gwgsvrNSTJF100UWl9YcffrjyvKfi5MmTHWuPP/546Wc/++yzSstkzQ4kQdiBJAg7kARhB5Ig7EAShB1IgrADSXCe/RywY8eOtlvoaN26dR1r3a61HxoaKq3fe++9lXoadAcPHiytP/XUU5Xm23XNbvtS27+x/b7tXbZ/WEyfafsV2x8WjzMqdQCgL6ayGX9C0tKImCvpryX9wPZcScskbYmIKyVtKV4DGFBdwx4RYxHxTvH8mKQPJF0iaYGkNcXb1ki6u0c9AmjAGe2z275M0jclbZM0FBGnd7oOSpp0B8v2EklLavQIoAFTPhpv+2uS1kt6KCL+MLEW43etnPRmkhGxKiJGImKkVqcAaplS2G1P03jQfxYRG4rJh2wPF/VhSYd70yKAJnS9lbRta3yf/GhEPDRh+r9I+t+IeMb2MkkzI+KxLvNKeSvpDRs2lNYXLFjQp05yOXHiRMfaqVOnas1706ZNpfXR0dHK837jjTdK61u3bi2td7qV9FT22f9G0j9Ies/29mLacknPSPqF7Qck7ZP0nSnMC0BLuoY9Iv5H0qT/Ukj6VrPtAOgVfi4LJEHYgSQIO5AEYQeSIOxAEgzZPAAee6z05wm1h3Quc/XVV5fWe3kZ6erVq0vre/furTX/9evXd6zt3r271rwHGUM2A8kRdiAJwg4kQdiBJAg7kARhB5Ig7EASnGcHzjGcZweSI+xAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkuobd9qW2f2P7fdu7bP+wmP6k7QO2txd/d/S+XQBVdb15he1hScMR8Y7tr0t6W9LdGh+P/XhE/OuUF8bNK4Ce63TziqmMzz4maax4fsz2B5IuabY9AL12Rvvsti+T9E1J24pJD9reYXu17RkdPrPE9qjt0XqtAqhjyvegs/01Sa9JeioiNtgeknREUkj6J41v6n+vyzzYjAd6rNNm/JTCbnuapF9J2hwR/zZJ/TJJv4qIv+oyH8IO9FjlG07atqSfSPpgYtCLA3enLZS0s26TAHpnKkfjb5b0hqT3JJ0qJi+XtFjSdRrfjN8r6fvFwbyyebFmB3qs1mZ8Uwg70HvcNx5IjrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5BE1xtONuyIpH0TXl9cTBtEg9rboPYl0VtVTfb2l50Kfb2e/SsLt0cjYqS1BkoMam+D2pdEb1X1qzc244EkCDuQRNthX9Xy8ssMam+D2pdEb1X1pbdW99kB9E/ba3YAfULYgSRaCbvt223/1vZHtpe10UMntvfafq8YhrrV8emKMfQO2945YdpM26/Y/rB4nHSMvZZ6G4hhvEuGGW/1u2t7+PO+77PbPk/S7yR9W9J+SW9JWhwR7/e1kQ5s75U0EhGt/wDD9t9KOi7p2dNDa9n+Z0lHI+KZ4h/KGRHx+ID09qTOcBjvHvXWaZjx76rF767J4c+raGPNfqOkjyLi44j4o6SfS1rQQh8DLyJel3T0S5MXSFpTPF+j8f9Z+q5DbwMhIsYi4p3i+TFJp4cZb/W7K+mrL9oI+yWSfj/h9X4N1njvIenXtt+2vaTtZiYxNGGYrYOShtpsZhJdh/Hupy8NMz4w312V4c/r4gDdV90cEddL+ntJPyg2VwdSjO+DDdK505WS5mh8DMAxST9qs5limPH1kh6KiD9MrLX53U3SV1++tzbCfkDSpRNezy6mDYSIOFA8Hpb0osZ3OwbJodMj6BaPh1vu5/9FxKGIOBkRpyT9WC1+d8Uw4+sl/SwiNhSTW//uJuurX99bG2F/S9KVtr9he7qkRZI2tdDHV9i+oDhwItsXSJqnwRuKepOk+4vn90v6ZYu9/IlBGca70zDjavm7a33484jo+5+kOzR+RH6PpH9so4cOfV0u6d3ib1fbvUl6QeObdV9o/NjGA5JmSdoi6UNJ/y1p5gD19pzGh/beofFgDbfU280a30TfIWl78XdH299dSV99+d74uSyQBAfogCQIO5AEYQeSIOxAEoQdSIKwA0kQdiCJ/wN8jzcem5JvKwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = data.squeeze(0).squeeze(0).cpu().numpy()\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
